# -*- coding: utf-8 -*-
"""Okee_bismillah.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15sWA88nNzAr2gORHhredwiNiubJ1mPfD
"""

# !pip install -q transformers datasets

import pandas as pd
import numpy as np
import torch
from transformers import TrainingArguments
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.utils.class_weight import compute_class_weight
from datasets import Dataset as HFDataset
from datasets import Dataset
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer, DataCollatorWithPadding
from sklearn.metrics import classification_report, accuracy_score

df = pd.read_csv('https://raw.githubusercontent.com/Frazanhibriz/Product_Review_Analysis/refs/heads/main/tokopedia_review.csv')
df.head()

# df.drop(columns=['Unnamed: 0', 'product_id', 'shop_id', 'product_url'], inplace=True)
# df.head()

# df["sold"] = df["sold"].fillna("0")
# df.isnull().sum()

# def label_sentiment(rating):
#     if rating >= 4:
#         return "positive"
#     elif rating == 3:
#         return "neutral"
#     else:
#         return "negative"

# df["sentiment"] = df["rating"].apply(label_sentiment)

# label_encoder = LabelEncoder()
# df["label"] = label_encoder.fit_transform(df["sentiment"])

# df.head()

# df = df[['text', 'rating', 'label']].dropna()
# df.head()

# train_texts, val_texts, train_labels, val_labels = train_test_split(
#     df['text'].tolist(),
#     df['label'].tolist(),
#     test_size=0.2,
#     random_state=42,
#     stratify=df['label']
# )

# tokenizer = BertTokenizer.from_pretrained("indobenchmark/indobert-base-p1")

# class TokopediaReviewDataset(Dataset):
#     def __init__(self, texts, labels, tokenizer, max_len=128):
#         self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_len)
#         self.labels = labels

#     def __getitem__(self, idx):
#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
#         item["labels"] = torch.tensor(self.labels[idx])
#         return item

#     def __len__(self):
#         return len(self.labels)

# train_dataset = TokopediaReviewDataset(train_texts, train_labels, tokenizer)
# val_dataset = TokopediaReviewDataset(val_texts, val_labels, tokenizer)

# model = BertForSequenceClassification.from_pretrained("indobenchmark/indobert-base-p1", num_labels=3)

# training_args = TrainingArguments(
#     output_dir="./results",
#     eval_strategy="epoch",
#     save_strategy="epoch",
#     learning_rate=2e-5,
#     per_device_train_batch_size=16,
#     per_device_eval_batch_size=64,
#     num_train_epochs=3,
#     weight_decay=0.01,
#     load_best_model_at_end=True,
#     metric_for_best_model="eval_loss"
# )

# preds_output = trainer.predict(val_dataset)
# y_pred = np.argmax(preds_output.predictions, axis=-1)
# print(classification_report(val_labels, y_pred, target_names=["negative", "neutral", "positive"]))

"""## Versi 2"""

df = pd.read_csv('https://raw.githubusercontent.com/Frazanhibriz/Product_Review_Analysis/refs/heads/main/tokopedia_review.csv')
df.head()

df.columns

df = df[['text', 'rating']].dropna()

def map_rating_to_label(rating):
    if rating <= 2:
        return 'negative'
    elif rating == 3:
        return 'neutral'
    else:
        return 'positive'

df['label'] = df['rating'].apply(map_rating_to_label)
df = df[['text', 'label']]
df.head()

le = LabelEncoder()
df['label_enc'] = le.fit_transform(df['label'])  # 0=negative, 1=neutral, 2=positive
label2id = {label: idx for idx, label in enumerate(le.classes_)}
id2label = {v: k for k, v in label2id.items()}

tokenizer = AutoTokenizer.from_pretrained("indobenchmark/indobert-base-p1")

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(), df['label_enc'].tolist(), test_size=0.2, stratify=df['label_enc'], random_state=42
)

train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})
val_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels})

def tokenize(batch):
    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)

# Explicitly set format to numpy first to avoid the copy issue
train_dataset.set_format(type='numpy', columns=['input_ids', 'attention_mask', 'label'])
val_dataset.set_format(type='numpy', columns=['input_ids', 'attention_mask', 'label'])

train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

model = AutoModelForSequenceClassification.from_pretrained(
    "indobenchmark/indobert-base-p1",
    num_labels=3,
    id2label=id2label,
    label2id=label2id
)

from transformers import TrainingArguments
help(TrainingArguments)

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=1)
    return {
        "accuracy": accuracy_score(labels, preds)
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics
)

trainer.train()

preds_output = trainer.predict(val_dataset)
preds = preds_output.predictions.argmax(axis=1)

print(classification_report(val_labels, preds, target_names=le.classes_))