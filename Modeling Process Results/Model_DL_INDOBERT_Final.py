# -*- coding: utf-8 -*-
"""Model Ringan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15JXWCOB-WBfjj3gaVTZzWdFpQZSihgJt
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE

# df = pd.read_csv('https://raw.githubusercontent.com/Frazanhibriz/Product_Review_Analysis/refs/heads/main/tokopedia_review.csv')
# df.head()

# def get_sentiment(rating):
#     if rating <= 2:
#         return 'negative'
#     elif rating == 3:
#         return 'neutral'
#     else:
#         return 'positive'

# df['sentiment'] = df['rating'].apply(get_sentiment)
# df = df[['text', 'sentiment']].dropna()

# X_train, X_test, y_train, y_test = train_test_split(
#     df['text'], df['sentiment'], test_size=0.2, stratify=df['sentiment'], random_state=42
# )

# tfidf = TfidfVectorizer(max_features=5000)
# X_train_tfidf = tfidf.fit_transform(X_train)
# X_test_tfidf = tfidf.transform(X_test)

# smote = SMOTE(random_state=42)
# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

# model = LogisticRegression(max_iter=1000)
# model.fit(X_train_resampled, y_train_resampled)

# y_pred = model.predict(X_test_tfidf)

# print("Classification Report:\n")
# print(classification_report(y_test, y_pred))

# conf_matrix = confusion_matrix(y_test, y_pred, labels=['positive', 'neutral', 'negative'])
# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
#             xticklabels=['positive', 'neutral', 'negative'],
#             yticklabels=['positive', 'neutral', 'negative'])
# plt.title("Confusion Matrix After SMOTE")
# plt.xlabel("Predicted")
# plt.ylabel("Actual")
# plt.show()

# from sklearn.utils.class_weight import compute_class_weight
# import numpy as np

# classes = ['positive', 'neutral', 'negative']
# weights = compute_class_weight(class_weight='balanced', classes=np.array(classes), y=y_train)
# class_weight_dict = {cls: w for cls, w in zip(classes, weights)}
# print("Class Weights:", class_weight_dict)

# from sklearn.preprocessing import LabelEncoder

# le = LabelEncoder()
# y_train_enc = le.fit_transform(y_train)
# y_test_enc = le.transform(y_test)

# sample_weights = np.array([class_weight_dict[label] for label in y_train])

# model = XGBClassifier(
#     objective='multi:softmax',
#     num_class=3,
#     eval_metric='mlogloss',
#     use_label_encoder=False,
#     random_state=42
# )

# model.fit(X_train_tfidf, y_train_enc, sample_weight=sample_weights)

# y_pred_enc = model.predict(X_test_tfidf)
# y_pred = le.inverse_transform(y_pred_enc)
# y_true = le.inverse_transform(y_test_enc)

# print("Classification Report:\n")
# print(classification_report(y_true, y_pred))

# conf_matrix = confusion_matrix(y_true, y_pred, labels=classes)
# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
#             xticklabels=classes, yticklabels=classes)
# plt.title("Confusion Matrix XGBoost")
# plt.xlabel("Predicted")
# plt.ylabel("Actual")
# plt.show()

"""## Versi 1"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
!pip install -q transformers datasets evaluate scikit-learn
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import evaluate

df = pd.read_csv("https://raw.githubusercontent.com/Frazanhibriz/Product_Review_Analysis/refs/heads/main/tokopedia_review_cleaned.csv")
df = df[['cleaned_text', 'sentiment']].dropna()
df.head()

label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['sentiment'])

train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

model_name = "indobenchmark/indobert-base-p1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

def tokenize_function(example):
    return tokenizer(example["cleaned_text"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    logging_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
)

accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")

def compute_metrics(p):
    preds = p.predictions.argmax(-1)
    labels = p.label_ids
    return {
        "accuracy": accuracy.compute(predictions=preds, references=labels)["accuracy"],
        "f1": f1.compute(predictions=preds, references=labels, average="weighted")["f1"]
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# âœ… 11. Training model
trainer.train()

metrics = trainer.evaluate()

print(f"Akurasi: {metrics['eval_accuracy']:.4f}")
print(f"F1-score: {metrics['eval_f1']:.4f}")

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

predictions = trainer.predict(test_dataset)
y_pred = predictions.predictions.argmax(-1)
y_true = predictions.label_ids

labels = label_encoder.classes_

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

model_path = "./sentiment_model"

model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

import pickle
with open(f"{model_path}/label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

from sklearn.metrics import classification_report

predictions = trainer.predict(test_dataset)
y_pred = predictions.predictions.argmax(-1)
y_true = predictions.label_ids

labels = label_encoder.classes_
report = classification_report(y_true, y_pred, target_names=labels)
print(report)

"""## Versi 2"""

import pandas as pd
import numpy as np
import re
import torch
from torch.utils.data import Dataset
from sklearn.utils import resample
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

df = pd.read_csv('https://raw.githubusercontent.com/Frazanhibriz/Product_Review_Analysis/refs/heads/main/tokopedia_review.csv')
df = df[['text', 'rating']].copy()

def convert_rating_to_sentiment(rating):
    if rating <= 2:
        return 'negative'
    elif rating == 3:
        return 'neutral'
    else:
        return 'positive'

df['sentimen'] = df['rating'].apply(convert_rating_to_sentiment)

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+", "", text)
    text = re.sub(r"[^a-zA-Z\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

df['text'] = df['text'].apply(clean_text)
df.dropna(subset=['text', 'sentimen'], inplace=True)

df_pos = df[df.sentimen == 'positive']
df_neu = df[df.sentimen == 'neutral']
df_neg = df[df.sentimen == 'negative']

df_neu_over = resample(df_neu, replace=True, n_samples=len(df_pos), random_state=42)
df_neg_over = resample(df_neg, replace=True, n_samples=len(df_pos), random_state=42)

df_balanced = pd.concat([df_pos, df_neu_over, df_neg_over]).sample(frac=1, random_state=42).reset_index(drop=True)

label_encoder = LabelEncoder()
df_balanced['label'] = label_encoder.fit_transform(df_balanced['sentimen'])

tokenizer = AutoTokenizer.from_pretrained("indobenchmark/indobert-base-p1")

class ReviewDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

texts = df_balanced['text'].tolist()
labels = df_balanced['label'].tolist()

X_train, X_eval, y_train, y_eval = train_test_split(texts, labels, test_size=0.2, random_state=42)

train_dataset = ReviewDataset(X_train, y_train, tokenizer)
eval_dataset = ReviewDataset(X_eval, y_eval, tokenizer)

model = AutoModelForSequenceClassification.from_pretrained("indobenchmark/indobert-base-p1", num_labels=3)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = torch.argmax(torch.tensor(logits), axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "precision": precision_score(labels, preds, average="macro"),
        "recall": recall_score(labels, preds, average="macro"),
        "f1": f1_score(labels, preds, average="macro"),
    }

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()

preds_output = trainer.predict(eval_dataset)
y_pred = torch.argmax(torch.tensor(preds_output.predictions), axis=1)
y_true = preds_output.label_ids

print("\nClassification Report:\n")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

df_raw = pd.read_csv('https://raw.githubusercontent.com/Frazanhibriz/Product_Review_Analysis/refs/heads/main/tokopedia_review.csv')
df_raw = df_raw[['text', 'rating']].copy()

df_raw['sentimen'] = df_raw['rating'].apply(convert_rating_to_sentiment)
df_raw['text'] = df_raw['text'].apply(clean_text)
df_raw.dropna(subset=['text', 'sentimen'], inplace=True)

df_test_external = df_raw.sample(n=100, random_state=123)

df_test_external['label'] = label_encoder.transform(df_test_external['sentimen'])

external_dataset = ReviewDataset(
    df_test_external['text'].tolist(),
    df_test_external['label'].tolist(),
    tokenizer
)

preds_external = trainer.predict(external_dataset)
y_pred_external = torch.argmax(torch.tensor(preds_external.predictions), axis=1)
y_true_external = preds_external.label_ids

print("Classification Report on External Data:\n")
print(classification_report(y_true_external, y_pred_external, target_names=label_encoder.classes_))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_true_external, y_pred_external)

labels = label_encoder.classes_

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix on External Data")
plt.tight_layout()
plt.show()

model.save_pretrained("sentiment_model")
tokenizer.save_pretrained("sentiment_model")